{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968091e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "          \"spark.pyspark.python\": \"python3\",\n",
    "          \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "          \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51671fbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"openpyxl==3.1.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e450b0",
   "metadata": {},
   "source": [
    "## 0. Imports and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945a7c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import boto3\n",
    "import os\n",
    "import sys\n",
    "from zipfile import ZipFile\n",
    "import openpyxl\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, when, to_date, max as spark_max, row_number, lit\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType\n",
    "\n",
    "print(\"INFO: Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a9209",
   "metadata": {},
   "source": [
    "## 1. Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89454d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "raw_zip_path     = \"s3://credit-risk-project-data/zipped/*.zip\"\n",
    "output_parquet   = \"s3://credit-risk-project-data/processed/train_dataset.parquet\"\n",
    "glossary_s3_path = \"s3://credit-risk-project-data/metadata/glossary.xlsx\"\n",
    "bucket_name      = \"credit-risk-project-data\"\n",
    "glossary_s3_key  = \"metadata/glossary.xlsx\"   \n",
    "local_glossary_path = \"glossary.xlsx\"         \n",
    "\n",
    "print(\"INFO: Configuration variables set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f4569",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(local_glossary_path):\n",
    "    print(f\"INFO: Downloading glossary from S3: s3://{bucket_name}/{glossary_s3_key} to {local_glossary_path}\")\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    try:\n",
    "        s3.download_file(bucket_name, glossary_s3_key, local_glossary_path)\n",
    "        print(\"INFO: Glossary downloaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: Failed to download glossary from S3. Error: {e}\")\n",
    "        sys.exit()\n",
    "else:\n",
    "    print(f\"INFO: Glossary file '{local_glossary_path}' already exists locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabccc5",
   "metadata": {},
   "source": [
    "## 2. Glossary Parsing and Schema Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da302352",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "column_names = []\n",
    "cast_types = {}\n",
    "parse_info = {}\n",
    "schema = StructType([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b82a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def parse_glossary(glossary_file_path):\n",
    "    loc_column_names = []\n",
    "    loc_cast_types_dict = {}\n",
    "    loc_parsing_info_dict = {}\n",
    "    \n",
    "    try:\n",
    "        workbook = openpyxl.load_workbook(glossary_file_path, data_only=True)\n",
    "        sheet = workbook.active\n",
    "            \n",
    "        if sheet:\n",
    "            for row_idx, row in enumerate(sheet.iter_rows(min_row=2)): # Start from the second row\n",
    "                if row_idx == 0 and all(cell.value is None for cell in row): # Skip if first data row is entirely blank\n",
    "                    print(\"INFO (parse_glossary): Skipping blank first row in glossary.\")\n",
    "                    continue\n",
    "\n",
    "                field = row[1].value   # Column Name\n",
    "                dtype = row[9].value   # Data Type\n",
    "                    \n",
    "                if not field:\n",
    "                    print(f\"DEBUG (parse_glossary): Skipping row '{row_idx+2}'  empty field name.\")\n",
    "                    continue\n",
    "                \n",
    "                clean_field_name = str(field).strip()\n",
    "                loc_column_names.append(clean_field_name)\n",
    "                    \n",
    "                post_cast_type = StringType() # Default post-cast type\n",
    "                    \n",
    "                if dtype and \"DATE\" in str(dtype).upper():\n",
    "                    post_cast_type = TimestampType()\n",
    "                    loc_parsing_info_dict[clean_field_name] = \"MMyyyy\"\n",
    "                elif dtype and \"ALPHA-NUMERIC\" in str(dtype).upper():\n",
    "                    post_cast_type = StringType()\n",
    "                elif dtype and \"ALPHA\" in str(dtype).upper():\n",
    "                    post_cast_type = StringType()\n",
    "                elif dtype and \"NUMERIC\" in str(dtype).upper():\n",
    "                    post_cast_type = DoubleType()\n",
    "                        \n",
    "                loc_cast_types_dict[clean_field_name] = post_cast_type\n",
    "        else:\n",
    "            print(\"WARNING (parse_glossary): Glossary sheet is not active or not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR (parse_glossary): Failed to parse glossary. Error: {e}\")\n",
    "    return loc_column_names, loc_cast_types_dict, loc_parsing_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f726cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "column_names, cast_types, parse_info = parse_glossary(local_glossary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265ebc1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Checking columns and building schema\n",
    "if column_names:\n",
    "    print(f\"INFO: Successfully parsed {len(column_names)} column names from glossary.\")\n",
    "    print(f\"INFO: column names: {column_names}\")\n",
    "    \n",
    "    # Define PySpark schema based on column_names\n",
    "    try:\n",
    "        schema_fields = [StructField(c_name, StringType(), True) for c_name in column_names]\n",
    "        schema = StructType(schema_fields)\n",
    "        print(\"\\nINFO: PySpark schema defined successfully based on glossary.\")\n",
    "        print(\"INFO: Schema structure:\")\n",
    "        for field in schema.fields:\n",
    "            print(f\"  Name: {field.name}, Type: {field.dataType}, Nullable: {field.nullable}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not create PySpark schema from column names. Error: {e}\")\n",
    "        column_names = [] # Invalidate column_names if schema creation fails\n",
    "        schema = StructType([])\n",
    "else:\n",
    "    print(\"CRITICAL WARNING: Glossary parsing resulted in an empty list of column names. DataFrame creation will likely fail or be incorrect.\")\n",
    "    schema = StructType([]) # Ensure schema is empty if column_names is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a309cc",
   "metadata": {},
   "source": [
    "## 3. SparkSession and SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542fe210",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "print(f\"INFO: SparkSession and SparkContext retrieved. Spark version: {sc.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7268cefe",
   "metadata": {},
   "source": [
    "## 4. Helper Functions for Data Extraction and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c63e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_rows(pds_bytes, source_zip_filename=\"Unknown.zip\"):\n",
    "    \"\"\"\n",
    "    pds_bytes: pure bytes (the content of a .zip file)\n",
    "    source_zip_filename: filename for logging\n",
    "    Yields each CSV row as a list of strings.\n",
    "    \"\"\"\n",
    "    buf = BytesIO(pds_bytes)\n",
    "    processed_any_csv = False\n",
    "    \n",
    "    try:\n",
    "        with ZipFile(buf) as zf:\n",
    "            csv_files_in_zip = [name for name in zf.namelist() if name.lower().endswith(\".csv\")]\n",
    "            if not csv_files_in_zip:\n",
    "                print(f\"WARNING (extract_rows): No CSV files found in zip: {source_zip_filename}\")\n",
    "                return\n",
    "\n",
    "            for name in csv_files_in_zip:\n",
    "                print(f\"INFO (extract_rows): Processing CSV '{name}' from zip '{source_zip_filename}'\")\n",
    "                with zf.open(name) as fh:\n",
    "                    line_count = 0\n",
    "                    empty_line_count = 0\n",
    "                    for raw_line_bytes in fh:\n",
    "                        try:\n",
    "                            line_str = raw_line_bytes.decode(\"ISO-8859-1\").rstrip()\n",
    "                            if not line_str.strip():\n",
    "                                empty_line_count += 1\n",
    "                                continue\n",
    "                            \n",
    "                            parts = line_str.split(\"|\")\n",
    "                            line_count += 1\n",
    "                            yield parts\n",
    "                            \n",
    "                            \n",
    "                        except UnicodeDecodeError as ude:\n",
    "                            print(f\"WARNING (extract_rows): UnicodeDecodeError in {name} from {source_zip_filename}, line approx {line_count+1}.\")\n",
    "                        except Exception as line_e:\n",
    "                            print(f\"WARNING (extract_rows): Error processing line {line_count+1} in {name} from {source_zip_filename}.\")\n",
    "                    \n",
    "                    if line_count == 0 and empty_line_count > 0:\n",
    "                        print(f\"INFO (extract_rows): CSV '{name}' in '{source_zip_filename}' contained only {empty_line_count} empty/blank lines.\")\n",
    "                    elif line_count == 0:\n",
    "                        print(f\"WARNING (extract_rows): CSV '{name}' in '{source_zip_filename}' was empty or yielded no data lines.\")\n",
    "                    else:\n",
    "                        print(f\"INFO (extract_rows): Processed {line_count} data lines (skipped {empty_line_count} empty lines) from '{name}'.\")\n",
    "                    processed_any_csv = True\n",
    "        if not processed_any_csv and csv_files_in_zip:\n",
    "             print(f\"WARNING (extract_rows): Found CSVs in {source_zip_filename} but none seemed to be processed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        print(f\"CRITICAL WARNING (extract_rows): Could not process zip content from '{source_zip_filename}'. Error Type: {error_type}, Error: {e}.\")\n",
    "        pass\n",
    "\n",
    "\n",
    "def robust_read_and_extract(path_pds_tuple):\n",
    "    \"\"\"\n",
    "    Safely gets bytes from pds_input (PortableDataStream or already bytes)\n",
    "    and then passes it to extract_rows.\n",
    "    \"\"\"\n",
    "    path, pds_input = path_pds_tuple\n",
    "    zip_filename = os.path.basename(path)\n",
    "    file_content_bytes = None\n",
    "\n",
    "    if hasattr(pds_input, 'read') and callable(getattr(pds_input, 'read')):\n",
    "        try:\n",
    "            file_content_bytes = pds_input.read()\n",
    "            print(f\"INFO (robust_read_and_extract): Successfully read {len(file_content_bytes) if file_content_bytes else 0} bytes for '{zip_filename}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR (robust_read_and_extract): Failed to .read() from PDS object for '{zip_filename}'. Error: {e}. Skipping this item.\")\n",
    "            return []\n",
    "    elif isinstance(pds_input, bytes):\n",
    "        print(f\"INFO (robust_read_and_extract): Input for '{zip_filename}' is already bytes. Length: {len(pds_input)}.\")\n",
    "        file_content_bytes = pds_input\n",
    "    else:\n",
    "        print(f\"ERROR (robust_read_and_extract): Unexpected type '{type(pds_input)}' for input associated with '{zip_filename}'. Skipping this item.\")\n",
    "        return []\n",
    "\n",
    "    if file_content_bytes is not None:\n",
    "        if not file_content_bytes:\n",
    "            print(f\"WARNING (robust_read_and_extract): File content for '{zip_filename}' is empty after read/conversion. Skipping.\")\n",
    "            return []\n",
    "        return extract_rows(file_content_bytes, source_zip_filename=zip_filename)\n",
    "    \n",
    "    print(f\"WARNING (robust_read_and_extract): No file content bytes obtained for '{zip_filename}'. Returning empty.\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def map_to_row_safe(parts_list): \n",
    "    \"\"\"\n",
    "    parts_list: list of strings from split row\n",
    "    \"\"\"\n",
    "    global column_names\n",
    "    if not column_names: # safeguard\n",
    "        print(\"CRITICAL (map_to_row_safe): column_names is empty. Cannot map to Row. Returning None.\")\n",
    "        return None\n",
    "\n",
    "    if len(parts_list) == len(column_names):\n",
    "        try:\n",
    "            return Row(*parts_list)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR (map_to_row_safe): Failed to create Row object. Parts: {str(parts_list)[:100]}. Error: {e}. Returning None.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"WARNING (map_to_row_safe): Column count mismatch. Data has {len(parts_list)} parts, schema expects {len(column_names)}.\")\n",
    "        return None\n",
    "\n",
    "print(\"INFO: Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948919b",
   "metadata": {},
   "source": [
    "## 5. Data Ingestion and RDD Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe8b2b",
   "metadata": {},
   "source": [
    "#### 5.1 - Read binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e1ee1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "zips_rdd_with_paths = sc.binaryFiles(raw_zip_path)\n",
    "\n",
    "initial_zip_files_count = zips_rdd_with_paths.count()\n",
    "print(f\"INFO: Stage 5.1 - Count of items from sc.binaryFiles (should be 20 files): {initial_zip_files_count}\")\n",
    "\n",
    "if initial_zip_files_count == 0:\n",
    "    print(\"CRITICAL: No zip files are being read by sc.binaryFiles. Check 'raw_zip_path' and S3 permissions.\")\n",
    "    rows_rdd = sc.emptyRDD()\n",
    "else:\n",
    "    print(f\"INFO: Stage 5.1 - Successfully found {initial_zip_files_count} zip file(s) to process.\")\n",
    "    if initial_zip_files_count > 0:\n",
    "        try:\n",
    "            print(\"INFO: Stage 5.1 - Checking types of first 3 PDS items (via map on executors):\")\n",
    "            collected_info = zips_rdd_with_paths.map(lambda path_pds_tuple: (path_pds_tuple[0], type(path_pds_tuple[1]).__name__)).take(3)\n",
    "            for path, type_name in collected_info:\n",
    "                print(f\"  File Path: {path}, Stream Object Type (as seen by Python map): {type_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: Stage 5.1 - Error during .take(3) for PDS type inspection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791fe6eb",
   "metadata": {},
   "source": [
    "#### 5.2 - flatMap to extract rows from CSVs within Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc8987",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rows_rdd = zips_rdd_with_paths.flatMap(robust_read_and_extract) # flatMap expects an iterable from the function\n",
    "\n",
    "count_rows_rdd = rows_rdd.count()\n",
    "print(f\"INFO: Stage 5.2 - Count of rows_rdd (raw string lists yielded from all CSVs via flatMap): {count_rows_rdd}\")\n",
    "\n",
    "if count_rows_rdd == 0:\n",
    "    print(\"CRITICAL WARNING: rows_rdd is empty. This means robust_read_and_extract / extract_rows did not yield any CSV rows.\")\n",
    "    print(\"  Check executor logs for 'INFO' or 'WARNING' messages from these functions.\")\n",
    "else:\n",
    "    print(\"INFO: Stage 5.2 - Sample data from rows_rdd (first 3 lists of strings):\")\n",
    "    for r_idx, r_val in enumerate(rows_rdd.take(3)):\n",
    "        print(f\"  Sample row {r_idx} (first 200 chars): {str(r_val)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75945d22",
   "metadata": {},
   "source": [
    "#### 5.3 - Map to Row objects and filter Nones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b32c0a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if 'rows_rdd' in locals() and count_rows_rdd > 0 and column_names and schema.fields:\n",
    "    print(\"INFO: Stage 5.3 - Starting map to Row objects.\")\n",
    "    rows_rdd_mapped = rows_rdd.map(map_to_row_safe)\n",
    "    \n",
    "    total_mapped_count = rows_rdd_mapped.count()\n",
    "    print(f\"INFO: Stage 5.3 - Count of rows_rdd_mapped (before filtering None): {total_mapped_count}\")\n",
    "    \n",
    "    rows_rdd_filtered = rows_rdd_mapped.filter(lambda r_obj: r_obj is not None)\n",
    "    final_rdd_for_df_count = rows_rdd_filtered.count()\n",
    "    \n",
    "    none_count = total_mapped_count - final_rdd_for_df_count\n",
    "    print(f\"INFO: Stage 5.3 - Number of rows that became None in map_to_row_safe (and were filtered): {none_count}\")\n",
    "    print(f\"INFO: Stage 5.3 - Count of rows_rdd_filtered (valid Row objects for DataFrame): {final_rdd_for_df_count}\")\n",
    "    \n",
    "    if final_rdd_for_df_count > 0:\n",
    "        print(\"INFO: Stage 5.3 - Sample data from rows_rdd_filtered (first 3 Row objects):\")\n",
    "        for r_obj_idx, r_obj_val in enumerate(rows_rdd_filtered.take(3)):\n",
    "            print(f\"  Sample Row object {r_obj_idx} (first 200 chars): {str(r_obj_val)[:200]}...\")\n",
    "    \n",
    "elif not column_names or not schema.fields:\n",
    "    print(\"CRITICAL WARNING: Stage 5.3 - Skipping map to Row objects because column_names or schema is empty/invalid.\")\n",
    "    final_rdd_for_df_count = 0 \n",
    "    rows_rdd_filtered = sc.emptyRDD()\n",
    "elif count_rows_rdd == 0 : \n",
    "     print(\"INFO: Stage 5.3 - Skipping map to Row objects as rows_rdd was empty.\")\n",
    "     final_rdd_for_df_count = 0\n",
    "     rows_rdd_filtered = sc.emptyRDD()\n",
    "else:\n",
    "    print(\"CRITICAL ERROR: Stage 5.3 - rows_rdd not defined.\")\n",
    "    final_rdd_for_df_count = 0\n",
    "    rows_rdd_filtered = sc.emptyRDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0900e334",
   "metadata": {},
   "source": [
    "## 6. Create Raw DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fed298",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if final_rdd_for_df_count > 0 and schema.fields:\n",
    "    print(f\"INFO: Stage 6 - Attempting to create df_raw from {final_rdd_for_df_count} rows using schema with {len(schema.fields)} fields.\")\n",
    "    try:\n",
    "        df_raw = spark.createDataFrame(rows_rdd_filtered, schema)\n",
    "        print(f\"INFO: df_raw successfully created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: Stage 6 - Failed to create DataFrame. Error: {e}\")\n",
    "        df_raw = spark.createDataFrame([], schema if schema.fields else StructType([]))\n",
    "else:\n",
    "    warning_msg = \"CRITICAL WARNING: Stage 6 - Cannot create df_raw because \"\n",
    "    if not final_rdd_for_df_count > 0: warning_msg += \"the filtered RDD is empty. \"\n",
    "    if not schema.fields: warning_msg += \"the schema has no fields. \"\n",
    "    print(warning_msg)\n",
    "    df_raw = spark.createDataFrame([], schema if schema.fields else StructType([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdfb81f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_raw_count = df_raw.count()\n",
    "print(f\"INFO: Stage 6 - Count of df_raw: {df_raw_count}\")\n",
    "\n",
    "if df_raw_count > 0:\n",
    "    print(\"INFO: Stage 6 - Schema of df_raw:\")\n",
    "    df_raw.printSchema()\n",
    "    print(\"INFO: Stage 6 - Sample data from df_raw (top 5 rows):\")\n",
    "    df_raw.show(5, truncate=False)\n",
    "else:\n",
    "    print(\"INFO: Stage 6 - df_raw is empty. Check previous logs for reasons (e.g., file read errors, all rows filtered by map_to_row_safe).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc38879",
   "metadata": {},
   "source": [
    "## 7. Initialize DataFrame 'df' for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c998f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = df_raw\n",
    "print(\"df_raw assigned to df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c9165",
   "metadata": {},
   "source": [
    "## 8. Type Casting and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb3ef7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if df_raw_count > 0:\n",
    "    # Cast each column according to cast_types\n",
    "    temp_df = df\n",
    "    for c_name, target_dtype in cast_types.items():\n",
    "        if c_name in temp_df.columns:\n",
    "            if isinstance(target_dtype, TimestampType):\n",
    "                fmt = parse_info.get(c_name)\n",
    "                if fmt:\n",
    "                    print(f\"INFO: Casting column '{c_name}' to TimestampType using format '{fmt}'.\")\n",
    "                    temp_df = temp_df.withColumn(c_name, to_date(col(c_name), fmt))\n",
    "                else:\n",
    "                    print(f\"WARNING: No parse_info format for date column '{c_name}'. Cannot cast to_date without format. Column remains string.\")\n",
    "            else:\n",
    "                print(f\"INFO: Casting column '{c_name}' to {target_dtype}.\")\n",
    "                temp_df = temp_df.withColumn(c_name, col(c_name).cast(target_dtype))\n",
    "        else:\n",
    "            print(f\"WARNING: Column '{c_name}' intended for casting not found in DataFrame. Available columns: {temp_df.columns}\")\n",
    "    df = temp_df\n",
    "\n",
    "    # Flag for delinquency\n",
    "    if \"Current Loan Delinquency Status\" in df.columns:\n",
    "        print(\"INFO: Creating 'IsDelinquent' column.\")\n",
    "        df = df.withColumn(\n",
    "            \"IsDelinquent\",\n",
    "            when(col(\"Current Loan Delinquency Status\") == \"XX\", 1)\n",
    "            .when(col(\"Current Loan Delinquency Status\").cast(\"int\") >= 3, 1)\n",
    "            .otherwise(0)\n",
    "        )\n",
    "    else:\n",
    "        print(\"WARNING: 'Current Loan Delinquency Status' column not found. Cannot create 'IsDelinquent'. Adding 'IsDelinquent' as 0.\")\n",
    "        df = df.withColumn(\"IsDelinquent\", lit(0)) # Add IsDelinquent as 0 if source column is missing\n",
    "\n",
    "    if \"Loan Age\" in df.columns:\n",
    "        print(\"INFO: Casting 'Loan Age' column to IntegerType.\")\n",
    "        df = df.withColumn(\"Loan Age\", col(\"Loan Age\").cast(IntegerType()))\n",
    "    else:\n",
    "        print(\"WARNING: 'Loan Age' column not found. Cannot cast.\")\n",
    "    print(\"INFO: Type casting and IsDelinquent creation stage complete.\")\n",
    "else:\n",
    "    print(\"INFO: Skipping type casting and feature engineering as input DataFrame 'df' is empty.\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c510f",
   "metadata": {},
   "source": [
    "## 9. Windowing and Feature/Label Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae3767",
   "metadata": {},
   "source": [
    "#### 9.1 - Filter for SF Origination Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c540d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if df_raw_count > 0: # Proceed only if df has data\n",
    "    # Window to pick first row per loan\n",
    "    if \"Loan Identifier\" in df.columns and \"Monthly Reporting Period\" in df.columns:\n",
    "        w_first = Window.partitionBy(\"Loan Identifier\").orderBy(col(\"Monthly Reporting Period\").asc())\n",
    "        print(\"INFO: Window 'w_first' defined.\")\n",
    "\n",
    "        # First-row features\n",
    "        static_cols_original = [ \n",
    "            \"Channel\",\"Seller Name\",\"Servicer Name\",\"Original Interest Rate\",\n",
    "            \"Original UPB\",\"Original Loan Term\",\"Original Loan to Value Ratio (LTV)\",\n",
    "            \"Original Combined Loan to Value Ratio (CLTV)\",\"Number of Borrowers\",\n",
    "            \"Debt-To-Income (DTI)\",\"Borrower Credit Score at Origination\",\n",
    "            \"Co-Borrower Credit Score at Origination\",\"First Time Home Buyer Indicator\",\n",
    "            \"Loan Purpose\",\"Property Type\",\"Number of Units\",\"Occupancy Status\",\n",
    "            \"Property State\",\"Metropolitan Statistical Area (MSA)\",\"Zip Code Short\",\n",
    "            \"Mortgage Insurance Percentage\",\"Amortization Type\",\n",
    "            \"Mortgage Insurance Type\",\"Special Eligibility Program\",\n",
    "            \"High Balance Loan Indicator\",\"Origination Date\"\n",
    "        ]\n",
    "        existing_static_cols = [c for c in static_cols_original if c in df.columns]\n",
    "        missing_static_cols = [c for c in static_cols_original if c not in df.columns]\n",
    "        if missing_static_cols:\n",
    "            print(f\"WARNING: These static columns for 'first_features' are not in df and will be excluded: {missing_static_cols}\")\n",
    "        \n",
    "        if existing_static_cols:\n",
    "            first_features = (\n",
    "                df\n",
    "                .withColumn(\"rn\", row_number().over(w_first))\n",
    "                .filter(col(\"rn\") == 1)\n",
    "                .select(\"Loan Identifier\", *existing_static_cols)\n",
    "            )\n",
    "            print(f\"INFO: 'first_features' created. Count: {first_features.count()}\")\n",
    "            first_features.show(3, truncate=False)\n",
    "        else:\n",
    "            print(\"WARNING: No existing columns found from the predefined list.\")\n",
    "            first_features = df.select(\"Loan Identifier\").distinct()\n",
    "            print(f\"INFO: Fallback 'first_features' created with only Loan Identifier (distinct). Count: {first_features.count()}\")\n",
    "\n",
    "    else:\n",
    "        print(\"WARNING: 'Loan Identifier' or 'Monthly Reporting Period' missing. Cannot define window or 'first_features'.\")\n",
    "else:\n",
    "    print(\"INFO: Skipping feature/label assembly as input DataFrame 'df' is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831113f",
   "metadata": {},
   "source": [
    "#### 9.2 - Create Default-in-12 Months Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa3866",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if df_raw_count > 0:\n",
    "    # 90 Day Delinquency (Default in first 12 Months)\n",
    "    if \"Loan Age\" in df.columns and \"Loan Identifier\" in df.columns and \"IsDelinquent\" in df.columns:\n",
    "        df_12 = df.filter(col(\"Loan Age\") <= 12)\n",
    "        label_df = (\n",
    "            df_12.groupBy(\"Loan Identifier\")\n",
    "            .agg(spark_max(\"IsDelinquent\").alias(\"Default_in_12M\"))\n",
    "        )\n",
    "        print(f\"INFO: 'label_df' created. Count: {label_df.count()}\")\n",
    "        label_df.show(3, truncate=False)\n",
    "    else:\n",
    "        print(\"WARNING: Columns needed for 'label_df' (Loan Age, Loan Identifier, IsDelinquent) are missing.\")\n",
    "else:\n",
    "    print(\"INFO: Skipping feature/label assembly as input DataFrame 'df' is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594ef6f",
   "metadata": {},
   "source": [
    "#### 9.3 - Calculate Max Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2322eb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if df_raw_count > 0:\n",
    "    if \"Loan Identifier\" in df.columns and \"Loan Age\" in df.columns:\n",
    "        age_df = (\n",
    "            df.groupBy(\"Loan Identifier\")\n",
    "            .agg(spark_max(\"Loan Age\").alias(\"LoanAge_max\"))\n",
    "        )\n",
    "        print(f\"INFO: 'age_df' created. Count: {age_df.count()}\")\n",
    "        age_df.show(3, truncate=False)\n",
    "        if age_df.count() > 0: age_df.select(\"LoanAge_max\").describe().show()\n",
    "    else:\n",
    "        print(\"WARNING: Columns needed for 'age_df' (Loan Identifier, Loan Age) are missing.\")\n",
    "else:\n",
    "    print(\"INFO: Skipping feature/label assembly as input DataFrame 'df' is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b520a",
   "metadata": {},
   "source": [
    "## 10. Final Join and Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f5b46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- Starting Final Join and Filter ---\")\n",
    "if df_raw_count > 0 and 'label_df' in locals() and 'first_features' in locals() and 'age_df' in locals():\n",
    "    intermediate_join = (\n",
    "        label_df\n",
    "        .join(first_features, \"Loan Identifier\", \"left\")\n",
    "        .join(age_df, \"Loan Identifier\", \"left\")\n",
    "    )\n",
    "    print(f\"INFO: 'intermediate_join' created. Count: {intermediate_join.count()}\")\n",
    "    intermediate_join.show(5, truncate=False)\n",
    "    \n",
    "    joined = intermediate_join\n",
    "    \n",
    "    # Filter by LoanAge_max first\n",
    "    if \"LoanAge_max\" in joined.columns:\n",
    "        print(\"INFO: Describing 'LoanAge_max' in intermediate_join:\")\n",
    "        joined.select(\"LoanAge_max\").describe().show()\n",
    "        \n",
    "        # Loans need to have at least 12 months of history\n",
    "        joined = joined.filter(col(\"LoanAge_max\") >= 12)\n",
    "        print(f\"INFO: Count of 'joined' DataFrame after LoanAge_max >= 12 filter: {joined.count()}\")\n",
    "    else:\n",
    "        print(\"WARNING: 'LoanAge_max' column not found in joined. Skipping LoanAge_max filter.\")\n",
    "    \n",
    "    # Apply origination date filter. Loans no earlier than Jan 2023\n",
    "    if \"Origination Date\" in joined.columns:\n",
    "        start_threshold_date = to_date(lit(\"012023\"), \"MMyyyy\")\n",
    "        joined = joined.filter(col(\"Origination Date\") >= threshold_date)\n",
    "        \n",
    "        print(f\"INFO: Count of final 'joined' DataFrame (after Origination Date >= {origination_date_threshold_str} filter): {joined.count()}\")\n",
    "    else:\n",
    "        print(\"WARNING: 'Origination Date' column not found in 'joined' DataFrame. Cannot apply Origination Date filter.\")\n",
    "\n",
    "    print(\"INFO: Final join and filter stage complete.\")\n",
    "    if 'joined' in locals() and joined.count() > 0:\n",
    "        joined.show(10, truncate=False)\n",
    "        try:\n",
    "            (joined.write\n",
    "            .mode(\"overwrite\")\n",
    "            .parquet(output_parquet))\n",
    "            \n",
    "            print(f\"SUCCESS: 'joined' DataFrame was successfully written to Parquet directory: {output_parquet}\")\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR: Failed to write 'joined' DataFrame to Parquet. Error: {e}\")\n",
    "    elif 'joined' in locals() and joined.count() == 0:\n",
    "        print(\"WARNING: Final 'joined' DataFrame is empty after all operations.\")\n",
    "        joined.show(10,truncate=False)\n",
    "    else:\n",
    "        print(\"ERROR: 'joined' DataFrame was not properly created or is unavailable.\")\n",
    "\n",
    "else:\n",
    "    print(\"INFO: Skipping final join and filter as one or more input DataFrames (df, label_df, first_features, age_df) were empty or not created.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
